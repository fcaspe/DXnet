# -*- coding: utf-8 -*-
"""DXNET_v6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HKBYQadiLnCmjEUw29x40A6hn7RoG07S
"""

# Commented out IPython magic to ensure Python compatibility.
import wave
import torch
import torch.nn as nn

import numpy as np

from dx7pytorch import dxsynth as dxs

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

'''
NETWORK DEFINITION
'''
class DXNET_block(nn.Module):
    def __init__(self, in_channels, out_channels,k,s,p):
        super().__init__()
        
        modules = []
        modules.append(nn.Conv1d(in_channels, out_channels, kernel_size=k,stride = s, padding=p))
        modules.append(nn.BatchNorm1d(out_channels))
        modules.append(nn.ReLU(inplace=True))
    
        self.block = nn.Sequential(*modules)
    
    def forward(self, x):
        #print("In: {}".format(x.shape))
        fx = self.block(x)
        #print(fx.shape)
        return fx

class DXNET(nn.Module):
    def __init__(self, output_dim, block, pool):
        super().__init__()
        
        self.features = nn.Sequential(
            block(1,256,k=512,s=4,p=254), #in_channels, out_channels,kernel_size,stride,padding
            block(256,64,k=64,s=2,p=31),
            pool(2),
            block(64,64,k=64,s=2,p=31),
            block(64,128,k=64,s=2,p=31),
            pool(2),
            block(128,128,k=64,s=2,p=32),
            block(128,256,k=64,s=2,p=31)
        )
        
        self.classifier = nn.Linear(31*256, output_dim)
        self.output_fn = torch.nn.Sigmoid()

    def forward(self, x):
        #print("Input: {}".format(x.shape))
        x = self.features(x)
        #print("Features: {}".format(x.shape))
        #Flatten the vector
        x = x.view(x.shape[0], -1)
        #print("Flattened: {}".format(x.shape))
        x = self.classifier(x)
        x = self.output_fn(x)
        return x

'''
Auxiliary patch functions
'''

def unnormalize_patch(y):
    maxes = torch.tensor( [
    99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, # osc6
    3, 3, 7, 3, 7, 99, 1, 31, 99, 14,
    99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, # osc5
    3, 3, 7, 3, 7, 99, 1, 31, 99, 14,
    99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, # osc4
    3, 3, 7, 3, 7, 99, 1, 31, 99, 14,
    99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, # osc3
    3, 3, 7, 3, 7, 99, 1, 31, 99, 14,
    99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, # osc2
    3, 3, 7, 3, 7, 99, 1, 31, 99, 14,
    99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, # osc1
    3, 3, 7, 3, 7, 99, 1, 31, 99, 14,
    99, 99, 99, 99, 99, 99, 99, 99, # pitch eg rate & level 
    31, 7, 1, 99, 99, 99, 99, 1, 5, 7, 48 # algorithm etc
    ] )
    yn = torch.mul(y,maxes)
    #print("y shape: {}".format(y.shape))
    #print("yn shape: {}".format(yn.shape))
    return yn

'''
Auxiliary audio functions
'''


def resample_and_mix(wf,t_start,t_len,target_fs,debug = False):
    '''
    Returns a numpy array of normalized float audio.
    '''
    if(debug):
        print("FrameRate: {} \n NChannels: {} \n SWidth: {}".format(wf.getframerate(),wf.getnchannels(),wf.getsampwidth()))
    byte_start = int( t_start * wf.getframerate() * wf.getnchannels() * wf.getsampwidth())
    byte_end = int( byte_start + t_len * wf.getframerate() * wf.getnchannels() * wf.getsampwidth() )
    
    bytes = wf.readframes(byte_end)
    if(len(bytes) < byte_end):
        print("ERROR: Wave file is shorter than specified! Byte Length is: {} - Byte Start: {} - Byte End: {}".format(len(bytes),byte_start,byte_end))
        quit()
    # Cut the part of audio we care for
    bytes = bytes[byte_start:byte_end]
    
    #  Convert to array for mixing.
    if(wf.getsampwidth() == 1):
        array = np.frombuffer(bytes,dtype=np.uint8)    
    if(wf.getsampwidth() == 2):
        array = np.frombuffer(bytes,dtype=np.int16)
    elif(wf.getsampwidth() == 4):
        array = np.frombuffer(bytes,dtype=np.float32)
    else:
        print("ERROR: Sample width not supported! Sample width is: {}".format(wf.getsampwidth))
        quit()

    # Mix the channels
    if(wf.getnchannels() == 2):
        audio = np.zeros(int(len(array)/2),dtype=np.float32)
        for i in range(len(audio)):
            audio[i] = float(array[2*i]) + float(array[2*i+1])
    elif(wf.getnchannels() == 1):
        audio = array.astype(float)
    else:
        print("ERROR: Number of channels not supported! N. Channels: {}".format(wf.getnchannels()))
        quit()
        
    # Resample
    step = float(wf.getframerate() / target_fs)
    
    resampled = np.zeros(int(len(audio)/step),dtype=np.float32)
    step_acc = 0
    for i in range(len(resampled)):
        resampled[i] = audio[int(step_acc)]
        step_acc += step
    
    # Normalize
    resampled /= np.max(resampled)
    
    return resampled

'''
CNN Evaluation
'''

def make_patch(model, device, audio):
    
    model = model.eval()
    synth = dxs.dxsynth(sampling_frequency=8000)
    with torch.no_grad():
    
        x = audio.reshape((1,1,len(audio)))
        x = torch.from_numpy(x)
        x = x.to(device)
    
        fx = model(x)
        fxu = torch.round(unnormalize_patch(fx))
        fxu = fxu.numpy()

        print("Extracted [alg {}]: \n{}".format(int(fxu[0,134]),fxu))
        
        #Room for patch corrections here
        # 1. Algorithm correction?
        # 2. Transpose correction?
        
        packed_patch = synth.pack_patch(fxu).reshape(1,128)
    
    return packed_patch

if __name__ == '__main__':

    wav_filename = './tests/test.wav'
    wf = wave.open(wav_filename,'rb')
    audio = resample_and_mix(wf,t_start= 0.0,t_len = 2.0,
        target_fs = 8000,debug=False)

    model_path = './model/dxnet.pt'
    model = torch.load(model_path,map_location=torch.device('cpu'))

    #Run Test assessment.
    make_patch(model,'cpu',audio)
